{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "import cv2\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make data  \n",
    "    #read file  \n",
    "input_dir = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train.txt' \n",
    "img_dir = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train'\n",
    "#     os.chdir(r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train')  \n",
    "with open(input_dir,'rb') as f:  \n",
    "    dirdata = [] \n",
    "\n",
    "    for line in f.readlines():  \n",
    "        lines = bytes.decode(line).strip().split('\\t')\n",
    "        lines = tuple(lines[0].split())\n",
    "        dirdata.append(lines)  \n",
    "dirdata = np.array(dirdata) \n",
    "#print(len(dirdata))\n",
    "#print(dirdata)  \n",
    "#read imgdata  \n",
    "imgdir,label_1 = zip(*dirdata)  \n",
    "alldata_x = []  \n",
    "for dirname in imgdir:\n",
    "    cur_filepath = img_dir + '\\\\' + dirname\n",
    "    #print(dirname)\n",
    "    img = cv2.imread(cur_filepath.strip(),cv2.IMREAD_COLOR)  \n",
    "    imgdata = cv2.resize(img,(224,224),cv2.INTER_LINEAR)  \n",
    "    alldata_x.append(imgdata)  \n",
    "#random shuffle  \n",
    "alldata = zip(alldata_x,label_1) \n",
    "#print(alldata)\n",
    "temp = list(alldata)  \n",
    "random.shuffle(temp)  \n",
    "data_xs,data_label = zip(*temp)  \n",
    "print(data_xs[0], type(data_xs[0]))\n",
    "data_x = np.asarray(data_xs)\n",
    "print(type(data_x))\n",
    "print(type(data_x[0]))\n",
    "print(data_label[0],type(data_label))\n",
    "label = [int(i) for i in data_label] \n",
    "print(label[0], type(label[0]))\n",
    "#label one hot  \n",
    "tf_label_onehot = tf.one_hot(label,100) \n",
    "print(tf_label_onehot)\n",
    "with tf.Session() as sess:  \n",
    "    data_y = sess.run(tf_label_onehot)\n",
    "\n",
    "#return data_x, data_y\n",
    "\n",
    "#data increase  \n",
    "def next_batch(step, data_x, data_y):\n",
    "    start = (step * BATCH_SIZE) % NUM_EXAMPLES\n",
    "    end= min(start + BATCH_SIZE, NUM_EXAMPLES)\n",
    "    train_x = data_x[start:end]  \n",
    "    train_y = data_y[start:end]  \n",
    "    #test_x = data_x[500:800]  \n",
    "    #test_y = data_y[500:800] \n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单个文件夹\n",
    "def read_img(path, textpath):\n",
    "    path = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train'\n",
    "    textpath = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train.txt'\n",
    "    dirdata = []\n",
    "    with open(textpath) as f:\n",
    "        line = f.readline()\n",
    "        #i = 0\n",
    "        while line:\n",
    "            linelist = line.split()\n",
    "            linelist[0] = path + '\\\\' + linelist[0]\n",
    "            dirdata.append(linelist)\n",
    "            line = f.readline()\n",
    "            #i = i+1 \n",
    "    #print(dirdata)\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    print(len(dirdata))\n",
    "    for data in dirdata:\n",
    "        #print('reading the images:%s'%(data[0]))\n",
    "        img=cv2.imread(data[0])\n",
    "        #img=cv2.resize(img,(w,h))\n",
    "        img = cv2.resize(img,(224,224),cv2.INTER_LINEAR)\n",
    "        imgs.append(img)\n",
    "        labels.append(data[1])\n",
    "    print(len(imgs))\n",
    "#     data = np.asarray(imgs, np.float32)\n",
    "#     label = np.asarray(labels, np.float32)\n",
    "    data = np.asarray(imgs, np.int32)\n",
    "    label = np.asarray(labels, np.int32)\n",
    "    tf_label_onehot = tf.one_hot(label,100)  #需要label为int\n",
    "    #print(tf_label_onehot)\n",
    "    with tf.Session() as sess:  \n",
    "        label = sess.run(tf_label_onehot)\n",
    "    #return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)\n",
    "     \n",
    "    return data, label\n",
    "def get_train_val_data(ratio=0.8):\n",
    "    data,label=read_img(path)\n",
    "\n",
    "\n",
    "    #打乱顺序\n",
    "    num_example=data.shape[0]\n",
    "    #print(num_example)\n",
    "    arr=np.arange(num_example)\n",
    "    np.random.shuffle(arr)\n",
    "    data=data[arr]\n",
    "    label=label[arr]\n",
    "\n",
    "    #将所有数据分为训练集和验证集\n",
    "    #ratio=0.8\n",
    "    s=np.int(num_example*ratio)\n",
    "    #print(type(num_example*ratio))\n",
    "    #print(type(s))\n",
    "    x_train=data[:s]\n",
    "    y_train=label[:s]\n",
    "    x_val=data[s:]\n",
    "    y_val=label[s:]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "#定义一个函数，按批次取数据\n",
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多个文件夹 没有one_hot\n",
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)]\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    for idx,folder in enumerate(cate):\n",
    "        for im in glob.glob(folder+'/*.jpg'):\n",
    "            print('reading the images:%s'%(im))\n",
    "            img=io.imread(im)\n",
    "            img=transform.resize(img,(w,h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)\n",
    "data,label=read_img(path)\n",
    "\n",
    "\n",
    "#打乱顺序\n",
    "num_example=data.shape[0]\n",
    "arr=np.arange(num_example)\n",
    "np.random.shuffle(arr)\n",
    "data=data[arr]\n",
    "label=label[arr]\n",
    "\n",
    "\n",
    "#将所有数据分为训练集和验证集\n",
    "ratio=0.8\n",
    "s=np.int(num_example*ratio)\n",
    "x_train=data[:s]\n",
    "y_train=label[:s]\n",
    "x_val=data[s:]\n",
    "y_val=label[s:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
