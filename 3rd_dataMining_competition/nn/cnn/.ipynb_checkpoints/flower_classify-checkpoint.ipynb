{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programfiles\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#from skimage import io,transform\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='e:/flower/'\n",
    "\n",
    "#将所有的图片resize成100*100\n",
    "w=100\n",
    "h=100\n",
    "c=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#读取图片\n",
    "def read_img(path):\n",
    "    cate=[path+x for x in os.listdir(path) if os.path.isdir(path+x)]\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    for idx,folder in enumerate(cate):\n",
    "        for im in glob.glob(folder+'/*.jpg'):\n",
    "            print('reading the images:%s'%(im))\n",
    "            img=io.imread(im)\n",
    "            img=transform.resize(img,(w,h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)\n",
    "data,label=read_img(path)\n",
    "\n",
    "\n",
    "#打乱顺序\n",
    "num_example=data.shape[0]\n",
    "arr=np.arange(num_example)\n",
    "np.random.shuffle(arr)\n",
    "data=data[arr]\n",
    "label=label[arr]\n",
    "\n",
    "\n",
    "#将所有数据分为训练集和验证集\n",
    "ratio=0.8\n",
    "s=np.int(num_example*ratio)\n",
    "x_train=data[:s]\n",
    "y_train=label[:s]\n",
    "x_val=data[s:]\n",
    "y_val=label[s:]\n",
    "\n",
    "#-----------------构建网络----------------------\n",
    "#占位符\n",
    "x=tf.placeholder(tf.float32,shape=[None,w,h,c],name='x')\n",
    "y_=tf.placeholder(tf.int32,shape=[None,],name='y_')\n",
    "\n",
    "#第一个卷积层（100——>50)\n",
    "conv1=tf.layers.conv2d(inputs=x,  filters=32,  kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu,\n",
    "      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "pool1=tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "#第二个卷积层(50->25)\n",
    "conv2=tf.layers.conv2d( inputs=pool1, filters=64,  kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu,\n",
    "      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "pool2=tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "#第三个卷积层(25->12)\n",
    "conv3=tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu,\n",
    "      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "pool3=tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "#第四个卷积层(12->6)\n",
    "conv4=tf.layers.conv2d(  inputs=pool3, filters=128, kernel_size=[3, 3], padding=\"same\",activation=tf.nn.relu,\n",
    "      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "pool4=tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "\n",
    "re1 = tf.reshape(pool4, [-1, 6 * 6 * 128])\n",
    "\n",
    "#全连接层\n",
    "dense1 = tf.layers.dense(inputs=re1,      units=1024,   activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                      kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "dense2= tf.layers.dense(inputs=dense1,  units=512,  activation=tf.nn.relu,\n",
    "                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                      kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "logits= tf.layers.dense(inputs=dense2,  units=5,   activation=None,\n",
    "                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "#---------------------------网络结束---------------------------\n",
    "\n",
    "loss=tf.losses.sparse_softmax_cross_entropy(labels=y_,logits=logits)\n",
    "train_op=tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(logits,1),tf.int32), y_)    \n",
    "acc= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "#定义一个函数，按批次取数据\n",
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练和测试数据，可将n_epoch设置更大一些\n",
    "\n",
    "n_epoch=1000\n",
    "batch_size=64\n",
    "sess=tf.InteractiveSession()  \n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #training\n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "        _,err,ac=sess.run([train_op,loss,acc], feed_dict={x: x_train_a, y_: y_train_a})\n",
    "        train_loss += err; train_acc += ac; n_batch += 1\n",
    "    print(\"   train loss: %f\" % (train_loss/ n_batch))\n",
    "    print(\"   train acc: %f\" % (train_acc/ n_batch))\n",
    "    \n",
    "    #validation\n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):\n",
    "        err, ac = sess.run([loss,acc], feed_dict={x: x_val_a, y_: y_val_a})\n",
    "        val_loss += err; val_acc += ac; n_batch += 1\n",
    "    print(\"   validation loss: %f\" % (val_loss/ n_batch))\n",
    "    print(\"   validation acc: %f\" % (val_acc/ n_batch))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单个文件夹\n",
    "def read_img(path, textpath):\n",
    "    path = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train'\n",
    "    textpath = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train.txt'\n",
    "    dirdata = []\n",
    "    with open(textpath) as f:\n",
    "        line = f.readline()\n",
    "        #i = 0\n",
    "        while line:\n",
    "            linelist = line.split()\n",
    "            linelist[0] = path + '\\\\' + linelist[0]\n",
    "            dirdata.append(linelist)\n",
    "            line = f.readline()\n",
    "            #i = i+1 \n",
    "    #print(dirdata)\n",
    "    imgs=[]\n",
    "    labels=[]\n",
    "    print(len(dirdata))\n",
    "    for data in dirdata:\n",
    "        #print('reading the images:%s'%(data[0]))\n",
    "        img=cv2.imread(data[0])\n",
    "        #img=cv2.resize(img,(w,h))\n",
    "        img = cv2.resize(img,(224,224),cv2.INTER_LINEAR)\n",
    "        imgs.append(img)\n",
    "        labels.append(data[1])\n",
    "    print(len(imgs))\n",
    "#     data = np.asarray(imgs, np.float32)\n",
    "#     label = np.asarray(labels, np.float32)\n",
    "    data = np.asarray(imgs, np.int32)\n",
    "    label = np.asarray(labels, np.int32)\n",
    "    tf_label_onehot = tf.one_hot(label,100)  #需要label为int\n",
    "    #print(tf_label_onehot)\n",
    "    with tf.Session() as sess:  \n",
    "        label = sess.run(tf_label_onehot)\n",
    "    #return np.asarray(imgs,np.float32),np.asarray(labels,np.int32)\n",
    "     \n",
    "    return data, label\n",
    "\n",
    "def get_train_val_data(ratio=0.8):\n",
    "    data,label=read_img(path)\n",
    "\n",
    "\n",
    "    #打乱顺序\n",
    "    num_example=data.shape[0]\n",
    "    #print(num_example)\n",
    "    arr=np.arange(num_example)\n",
    "    np.random.shuffle(arr)\n",
    "    data=data[arr]\n",
    "    label=label[arr]\n",
    "\n",
    "    #将所有数据分为训练集和验证集\n",
    "    #ratio=0.8\n",
    "    s=np.int(num_example*ratio)\n",
    "    #print(type(num_example*ratio))\n",
    "    #print(type(s))\n",
    "    x_train=data[:s]\n",
    "    y_train=label[:s]\n",
    "    x_val=data[s:]\n",
    "    y_val=label[s:]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2725\n",
      "2725\n",
      "Tensor(\"one_hot_4:0\", shape=(2725, 100), dtype=float32)\n",
      "<class 'numpy.ndarray'> [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "path = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train'\n",
    "textpath = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train.txt'\n",
    "#data, label = read_img(path, textpath)\n",
    "dirdata = []\n",
    "with open(textpath) as f:\n",
    "    line = f.readline()\n",
    "    #i = 0\n",
    "    while line:\n",
    "        linelist = line.split()\n",
    "        linelist[0] = path + '\\\\' + linelist[0]\n",
    "        dirdata.append(linelist)\n",
    "        line = f.readline()\n",
    "        #i = i+1 \n",
    "#print(dirdata)\n",
    "imgs=[]\n",
    "labels=[]\n",
    "print(len(dirdata))\n",
    "for data in dirdata:\n",
    "    #print('reading the images:%s'%(data[0]))\n",
    "    img=cv2.imread(data[0])\n",
    "    #img=cv2.resize(img,(w,h))\n",
    "    img = cv2.resize(img,(224,224),cv2.INTER_LINEAR)\n",
    "    imgs.append(img)\n",
    "    labels.append(data[1])\n",
    "print(len(imgs))\n",
    "# data = np.asarray(imgs, np.float32)\n",
    "# label = np.asarray(labels, np.float32)\n",
    "data = np.asarray(imgs, np.float32)\n",
    "label = np.asarray(labels, np.float32)\n",
    "tf_label_onehot = tf.one_hot(label,100)  #需要label为int\n",
    "print(tf_label_onehot)\n",
    "with tf.Session() as sess:  \n",
    "    data_y = sess.run(tf_label_onehot)\n",
    "print(type(data_y), data_y[0], type(data_y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train'\n",
    "img=cv2.imread(path + '\\\\' + '0b7b02087bf40ad1aa2ba0685d2c11dfa8ecce6a.jpg')\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(['a', 'b', 'c', 'd', 'e'])\n",
    "#dlist = ['a', 'b', 'c', 'd', 'e']\n",
    "label = np.asarray([1, 2, 3, 4, 5])\n",
    "#dlabel = [1, 2, 3, 4, 5]\n",
    "num_example=data.shape[0]\n",
    "#num_example=dlist.shape[0]\n",
    "print(num_example)\n",
    "arr=np.arange(num_example)\n",
    "np.random.shuffle(arr)\n",
    "data=data[arr]\n",
    "label=label[arr]\n",
    "print(arr)\n",
    "print(data)\n",
    "print(label)\n",
    "#实验证明，only integer scalar arrays can be converted to a scalar index\n",
    "# dlist = dlist[arr]\n",
    "# dlabel = dlabel[arr]\n",
    "# print(arr)\n",
    "# print(dlist)\n",
    "# print(alabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
