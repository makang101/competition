{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programfiles\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import tensorflow as tf\n",
    "img_dir = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train'\n",
    "text_dir =  r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train.txt' \n",
    "BATCH_SIZE = 32\n",
    "NUM_EXAMPLES = 2725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = tf.Variable(tf.constant(1.0, shape=[1]), name = 'v1')\n",
    "v2 = tf.Variable(tf.constant(2.0, shape=[1]), name = 'v2')\n",
    "result = v1 + v2\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    saver.save(sess, 'model/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "[3.]\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(tf.constant(1.0, shape=[1]), name = '0_v1')\n",
    "v2 = tf.Variable(tf.constant(2.0, shape=[1]), name = '0_v2')\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(init_op)\n",
    "    saver.restore(sess, 'model/model.ckpt')\n",
    "    print(sess.run(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train.txt'  \n",
    "os.chdir(r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train')\n",
    "with open(file,'rb') as f:\n",
    "    dirdata = []\n",
    "    i = 0\n",
    "    for line in f.readlines(): \n",
    "        if i > 3: \n",
    "            break\n",
    "        else:\n",
    "            i = i + 1\n",
    "        lines = bytes.decode(line).strip().split('\\t')\n",
    "        lines = tuple(lines[0].split())\n",
    "        dirdata.append(lines) \n",
    "    print(dirdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_img(input_dir):\n",
    "#make data  \n",
    "    #read file  \n",
    "#     file = r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train.txt'  \n",
    "#     os.chdir(r'D:\\Project\\jupyter\\project\\nn\\cnn\\datasets\\train')  \n",
    "with open(text_dir,'rb') as f:  \n",
    "    dirdata = [] \n",
    "\n",
    "    for line in f.readlines():  \n",
    "        lines = bytes.decode(line).strip().split('\\t')\n",
    "        lines = tuple(lines[0].split())\n",
    "        dirdata.append(lines)  \n",
    "dirdata = np.array(dirdata) \n",
    "print(len(dirdata))\n",
    "#print(dirdata)  \n",
    "#read imgdata  \n",
    "imgdir,label_1 = zip(*dirdata)  \n",
    "alldata_x = []  \n",
    "for dirname in imgdir:\n",
    "    cur_filepath = img_dir + '\\\\' + dirname\n",
    "    #print(dirname)\n",
    "    img = cv2.imread(cur_filepath.strip(),cv2.IMREAD_COLOR)  \n",
    "    imgdata = cv2.resize(img,(224,224),cv2.INTER_LINEAR)  \n",
    "    alldata_x.append(imgdata)  \n",
    "#random shuffle  \n",
    "alldata = zip(alldata_x,label_1) \n",
    "print(alldata)\n",
    "temp = list(alldata)  \n",
    "random.shuffle(temp)  \n",
    "data_xs,data_label = zip(*temp)  \n",
    "print(data_xs[0])\n",
    "#print(type(data_xs[0]))\n",
    "data_x = np.array(data_xs)\n",
    "#print(type(data_x))\n",
    "label = [int(i) for i in data_label]  \n",
    "#label one hot  \n",
    "tf_label_onehot = tf.one_hot(label,100) \n",
    "#print(tf_label_onehot)\n",
    "with tf.Session() as sess:  \n",
    "    data_y = sess.run(tf_label_onehot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data increase  \n",
    "def next_batch(step, data_x, data_y):\n",
    "    start = (step * BATCH_SIZE) % NUM_EXAMPLES\n",
    "    end= min(start + BATCH_SIZE, NUM_EXAMPLES)\n",
    "    train_x = data_x[start:end]  \n",
    "    train_y = data_y[start:end]  \n",
    "    test_x = data_x[500:800]  \n",
    "    test_y = data_y[500:800] \n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next_batch(1, data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import tensorflow as tf  \n",
    "import os  \n",
    "import random  \n",
    "  \n",
    "def convinit(w,h,channel,featurenum):  \n",
    "    W = tf.Variable(tf.truncated_normal([w,h,channel,featurenum],stddev = 0.01))#首先需要创建W和b变量  \n",
    "    b = tf.Variable(tf.constant(0.01,shape = [featurenum]))  \n",
    "    return W,b  \n",
    "def fcinit(inputD,outputD):  \n",
    "    W = tf.Variable(tf.truncated_normal([inputD,outputD],stddev =0.01),dtype = tf.float32)  \n",
    "    b = tf.Variable(tf.constant(0.01,shape = [outputD]),dtype = tf.float32)  \n",
    "    return W,b  \n",
    "def convLayer(x,W,b,stride_x,stride_y,Flagure,padding = 'SAME'):  \n",
    "    conv = tf.nn.conv2d(x,W,strides = [1,stride_x,stride_y,1],padding = padding)#进行卷积处理  \n",
    "    out = tf.add(conv,b)  \n",
    "    if Flagure:  \n",
    "        return tf.nn.relu(out)  \n",
    "    else:  \n",
    "        return out #在最后一个卷积时不需要用relu  \n",
    "def LRN(x,alpha,beta,R,bias):  \n",
    "    y = tf.nn.local_response_normalization(x,depth_radius = R,alpha = alpha,beta = beta,bias = bias)  \n",
    "    return y   \n",
    "def max_poolLayer(x,w,h,stride_x,stride_y,padding = 'SAME'):  \n",
    "    y = tf.nn.max_pool(x,ksize = [1,w,h,1],strides = [1,stride_x,stride_y,1],padding = padding)  \n",
    "    return y  \n",
    "def dropout(x,keeppro):  \n",
    "    y = tf.nn.dropout(x,keeppro)  \n",
    "    return y  \n",
    "def fcLayer(x,W,b,Flagure):  \n",
    "    out = tf.add(tf.matmul(x,W),b)  \n",
    "    if Flagure:  \n",
    "        return tf.nn.relu(out)  \n",
    "    else:  \n",
    "        return out  \n",
    "def model(x,keeppro):  \n",
    "    #conv1  \n",
    "    W1,b1 = convinit(10,10,3,64)  \n",
    "    conv1 = convLayer(x,W1,b1,4,4,True,'VALID')  \n",
    "    LRN1 = LRN(conv1,2e-05,0.75,2,1)  \n",
    "    maxpool1 = max_poolLayer(LRN1,3,3,2,2,'VALID')  \n",
    "    #conv2  \n",
    "    W2,b2 = convinit(5,5,64,96)  \n",
    "    conv2 = convLayer(maxpool1,W2,b2,2,2,True,'VALID')  \n",
    "    LRN2 = LRN(conv2,2e-05,0.75,2,1)  \n",
    "    maxpool2 = max_poolLayer(LRN2,3,3,2,2,'VALID')  \n",
    "    #conv3  \n",
    "    W3,b3 = convinit(3,3,96,128)  \n",
    "    conv3 = convLayer(maxpool2,W3,b3,1,1,True,'SAME')  \n",
    "    #conv4  \n",
    "    W4,b4 = convinit(3,3,128,256)  \n",
    "    conv4 = convLayer(conv3,W4,b4,1,1,True,'SAME')  \n",
    "    #conv5  \n",
    "    W5,b5 = convinit(3,3,256,256)  \n",
    "    conv5 = convLayer(conv4,W5,b5,1,1,True,'SAME')  \n",
    "    maxpool5 = max_poolLayer(conv5,2,2,2,2,'SAME')  \n",
    "    #fclayer1  \n",
    "    fcIn = tf.reshape(maxpool5,[-1,4*4*256])  \n",
    "    W_1,b_1 = fcinit(4*4*256,512)  \n",
    "    fcout1 = fcLayer(fcIn,W_1,b_1,True)  \n",
    "    dropout1 = dropout(fcout1,keeppro)  \n",
    "    #fclayer2  \n",
    "    W_2,b_2 = fcinit(512,256)  \n",
    "    fcout2 = fcLayer(dropout1,W_2,b_2,True)  \n",
    "    dropout2 = dropout(fcout2,keeppro)  \n",
    "    #fclayer3  \n",
    "    W_3,b_3 = fcinit(256,10)  \n",
    "    fcout3 = fcLayer(dropout2,W_3,b_3,False)      \n",
    "    out_1 = tf.nn.softmax(fcout3)  \n",
    "    out = dropout(out_1,keeppro)  \n",
    "    return out   \n",
    "def accuracy(x,y):  \n",
    "    global out  \n",
    "    predict = sess.run(out,feed_dict = {x:test_x,keeppro:0.5})  \n",
    "    correct_predict = tf.equal(tf.argmax(predict,1),tf.argmax(y,1))  \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict,tf.float32))  \n",
    "    result = sess.run(accuracy,feed_dict = {x:test_x,y:test_y,keeppro:0.5})  \n",
    "    return predict,result   \n",
    "  \n",
    "#make data  \n",
    "#read file  \n",
    "file = 'D:\\\\CNN paper\\\\Alex_net\\\\image1000test200\\\\train.txt'  \n",
    "os.chdir('D:\\\\CNN paper\\\\Alex_net\\\\image1000test200\\\\train')  \n",
    "with open(file,'rb') as f:  \n",
    "    dirdata = []  \n",
    "    for line in f.readlines():  \n",
    "        lines = bytes.decode(line).strip().split('\\t')  \n",
    "        dirdata.append(lines)  \n",
    "dirdata = np.array(dirdata)  \n",
    "  \n",
    "#read imgdata  \n",
    "imgdir,label_1 = zip(*dirdata)  \n",
    "alldata_x = []  \n",
    "for dirname in imgdir:  \n",
    "    img = cv2.imread(dirname.strip(),cv2.IMREAD_COLOR)  \n",
    "    imgdata = cv2.resize(img,(320,320),cv2.INTER_LINEAR)  \n",
    "    alldata_x.append(imgdata)  \n",
    "#random shuffle  \n",
    "alldata = zip(alldata_x,label_1)  \n",
    "temp = list(alldata)  \n",
    "random.shuffle(temp)  \n",
    "data_xs,data_label = zip(*temp)  \n",
    "data_x = np.array(data_xs)  \n",
    "label = [int(i) for i in data_label]  \n",
    "#label one hot  \n",
    "tf_label_onehot = tf.one_hot(label,10)  \n",
    "with tf.Session() as sess:  \n",
    "    data_y = sess.run(tf_label_onehot)  \n",
    "#data increase  \n",
    "train_x = data_x[:500]  \n",
    "train_y = data_y[:500]  \n",
    "test_x = data_x[500:800]  \n",
    "test_y = data_y[500:800]  \n",
    "  \n",
    "x = tf.placeholder(tf.float32,[None,320,320,3])  \n",
    "y = tf.placeholder(tf.float32,[None,10])  \n",
    "keeppro = tf.placeholder(tf.float32)  \n",
    "out = model(x,keeppro)  \n",
    "out = tf.clip_by_value(out,1e-10,1.0)  \n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(out),reduction_indices = [1]))  \n",
    "Optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)  \n",
    "init = tf.global_variables_initializer()  \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)  \n",
    "    for i in range(100):  \n",
    "        sess.run(Optimizer,feed_dict = {x:train_x,y:train_y,keeppro:0.5})  \n",
    "        if i%10 == 0:  \n",
    "            cost = sess.run(loss,feed_dict = {x:train_x,y:train_y,keeppro:0.5})  \n",
    "            print('after %d iteration,cost is %f'%(i,cost))  \n",
    "            predict = sess.run(out,feed_dict = {x:test_x,keeppro:0.5})  \n",
    "            correct_predict = tf.equal(tf.argmax(predict,1),tf.argmax(y,1))  \n",
    "\n",
    "            \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict,tf.float32))  \n",
    "result = sess.run(accuracy,feed_dict = {x:test_x,y:test_y,keeppro:0.5})  \n",
    "print('after %d iteration,accuracy is %f'%(i,result))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
